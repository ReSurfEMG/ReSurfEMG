{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning\n",
    "This notebook is under development- please use to evaluate entropy notebook and suggest desired changes only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on amplitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note this notebook can only be run completely if you   install the picard library. Installation is not automatic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import find_peaks\n",
    "from collections import namedtuple\n",
    "import builtins\n",
    "import math\n",
    "from sklearn.decomposition import FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resurfemg.helper_functions as hf\n",
    "from resurfemg.tmsisdk_lite import Poly5Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure our data\n",
    "from resurfemg.config import Config\n",
    "config = Config()\n",
    "root_emg_directory = config.get_directory('root_emg_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here is the picard , you need to uncomment this cell and run\n",
    "#from picard import picard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set a collection place for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not rerun this cell\n",
    "big_data_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below change the path to the root directory where you are keeping your EMGs and ventilator \"Draeger\" files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reruns should be done from this cell as the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_pattern = os.path.join(root_emg_directory, '**/*.Poly5')\n",
    "emg_and_draeger_files = glob.glob(emg_pattern, recursive=True)\n",
    "\n",
    "emg_files = []\n",
    "draeger_files = []\n",
    "\n",
    "for file in emg_and_draeger_files:\n",
    "    if 'Draeger' in file:\n",
    "        draeger_files.append(file)\n",
    "    else:\n",
    "        emg_files.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can pick a file from the list, which have been numbered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_numbers_strung = []\n",
    "for i in range(len(emg_files)):\n",
    "    list_of_numbers_strung.append(str(i))\n",
    "\n",
    "\n",
    "btn = widgets.Dropdown(\n",
    "    options=list_of_numbers_strung,\n",
    "    value='0',\n",
    "    description='Picked File:',\n",
    "    disabled=False,\n",
    ")\n",
    "display(btn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caution! \n",
    "If you folder is set up in any way different then the picked file numbers will not neccesarily correspond to the same file. Always check the print out for the file you have chosen in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_chosen = int(btn.value)\n",
    "file_chosen = emg_files[number_chosen] \n",
    "print(\"The file you chose is:\",file_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_emg = Poly5Reader(file_chosen)\n",
    "data_samples= data_emg.samples\n",
    "emg_fs = data_emg.sample_rate\n",
    "converted_to_seconds =  []\n",
    "converted_to_samples = []\n",
    "for i in range(len(data_samples[0])):\n",
    "    converted_to_seconds.append(i/emg_fs)\n",
    "    converted_to_samples.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# set up plotn\n",
    "x = data_samples\n",
    "fig, axis = plt.subplots(nrows = 3, ncols = 2, figsize=(6, 6))\n",
    "#ax.set_ylim([-4, 4])\n",
    "axis[0,0].grid(True)\n",
    "axis[0,0].plot(x[0])\n",
    "axis[0,0].set(title='leads in samples')\n",
    "axis[1,0].plot(x[1])\n",
    "axis[2,0].plot(x[2])\n",
    "axis[0,1].set(title='leads in seconds')\n",
    "axis[0,1].grid(True)\n",
    "axis[0,1].plot(converted_to_seconds,x[0])\n",
    "axis[1,1].plot(converted_to_seconds,x[1])\n",
    "axis[2,1].plot(converted_to_seconds,x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the whole unfiltered EMG, but you probably want to examine a part. You will also want to examine something filtered down to only the EMG components. Therefore we will filter off only the EMG components with an ICA in addtion to the filter we will play with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can filter down to which part you want to see. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to cut and see the file in samples or seconds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_axis = widgets.Dropdown(\n",
    "    options=[\"Samples\",\"Seconds\"],\n",
    "    value='Samples',\n",
    "    description=\"Select View Option\",\n",
    "    disabled=False,\n",
    ")\n",
    "display(y_axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_view= y_axis.value\n",
    "time_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will pick the start and end of your sample.We are going to clip the end of the sample in processing, so you can not pick any values and get a good graph. We preset the values towards the max graphable with ease, but they can be overwritten.  In the future we will have an updating graph here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_view == 'Samples':\n",
    "    int_slider1 = widgets.IntSlider(\n",
    "        min=0, max=int(len(x[0])*0.89), step=1,\n",
    "        description=' samples start'\n",
    "    )\n",
    "    int_slider2 = widgets.IntSlider(\n",
    "        value=len(x[0]),\n",
    "        min=0, max=int(len(x[0])*0.89), step=1,\n",
    "        description='samples end cutoff'\n",
    "    )\n",
    "else:\n",
    "    int_slider1 = widgets.IntSlider(\n",
    "        #value=0.1,\n",
    "        min=0, max= int(converted_to_seconds[-1])*0.89, step=1,\n",
    "        description='seconds start'\n",
    "    )\n",
    "    int_slider2 = widgets.IntSlider(\n",
    "        #value=converted_to_seconds[-1],\n",
    "        min=0, max=int(converted_to_seconds[-1])*0.89, step= 1,\n",
    "        description='seconds end cutoff'\n",
    "    )\n",
    "    \n",
    "widgets.VBox(\n",
    "    [\n",
    "\n",
    "        int_slider1,\n",
    "        int_slider2,\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can overwrite the values by hand in the next cell, if scrolling is not precise enough...but rewriting to take an absolute end is unadvisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Here we can overwrite the values by hand, again you must pick values a bit inside\n",
    "int_slider1.value = 5000\n",
    "int_slider2.value = 70000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we will graph your choice in the next active cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int_slider1.value\n",
    "end= int_slider2.value\n",
    "if time_view == 'Samples':\n",
    "    # nox examine at a certain scale- from point a to b as samples\n",
    "    x = data_samples\n",
    "    fig, (ax_1,ax_2,ax_3) = plt.subplots(nrows = 3, figsize=(6, 4))\n",
    "    ax_1.grid(True)\n",
    "    ax_1.plot(x[0][int(start):int(end)])\n",
    "    ax_1.set(title='leads, samples')\n",
    "    ax_2.plot(x[1][int(start):int(end)])\n",
    "    ax_3.plot(x[2][int(start):int(end)])\n",
    "    \n",
    "if time_view == 'Seconds':\n",
    "    # nox examine at a certain scale- from point a to b as samples\n",
    "    x_for_secs = data_samples\n",
    "\n",
    "    y = converted_to_seconds\n",
    "    fig, (ax_1,ax_2,ax_3) = plt.subplots(nrows = 3, figsize=(6, 4))\n",
    "    ax_1.grid(True)\n",
    "    ax_1.plot(y[int(start*emg_fs):int(end*emg_fs)],x[0][int(start*emg_fs):int(end*emg_fs)])\n",
    "    ax_1.set(title='leads, seconds')\n",
    "    ax_2.plot(y[int(start*emg_fs):int(end*emg_fs)],x[1][int(start*emg_fs):int(end*emg_fs)])\n",
    "    ax_3.plot(y[int(start*emg_fs):int(end*emg_fs)],x[2][int(start*emg_fs):int(end*emg_fs)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def working_pipeline_exp(our_chosen_file):\n",
    "  \n",
    "    cut_file_data = bad_end_cutter(\n",
    "        our_chosen_file,\n",
    "        percent_to_cut=3,\n",
    "        tolerance_percent=5,\n",
    "    )\n",
    "    bd_filtered_file_data = emg_bandpass_butter_sample(\n",
    "        cut_file_data,\n",
    "        5,\n",
    "        450,\n",
    "        2048,\n",
    "        output='sos',\n",
    "    )\n",
    "    # end-cutting again to get rid of filtering artifacts\n",
    "    re_cut_file_data = bad_end_cutter_for_samples(\n",
    "        bd_filtered_file_data,\n",
    "        percent_to_cut=3,\n",
    "        tolerance_percent=5,\n",
    "    )\n",
    "    # do ICA\n",
    "    components = compute_ICA_two_comp(re_cut_file_data)\n",
    "    #  pick components with more peaj\n",
    "    emg = pick_more_peaks_array(components)\n",
    "    # now process it in final steps\n",
    "    abs_values = abs(emg)\n",
    "    final_envelope_d = emg_highpass_butter(abs_values, 150, 2048)\n",
    "    final_envelope_a = naive_rolling_rms(final_envelope_d, 300)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emg = hf.emg_bandpass_butter_sample(\n",
    "        data_samples,\n",
    "        5,\n",
    "        450,\n",
    "        2048,\n",
    "        output='sos',\n",
    "    )\n",
    "print(np.max(emg[0]), np.min(emg[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Her we print out max and min after a bandpass_butter_sample filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(emg[0][0:150000]),np.min(emg[0][0:150000]))\n",
    "print(np.max(emg[1][0:150000]),np.min(emg[1][0:150000]))\n",
    "print(np.max(emg[2][0:150000]),np.min(emg[2][0:150000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get precise about the amplitudes we think are going into the ICA now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lead 0 has\", np.max(emg[0][0:150000])-np.min(emg[0][0:150000]), \"variance in amplitude\")\n",
    "print(\"Lead 1 has\", np.max(emg[1][0:150000])-np.min(emg[1][0:150000]), \"variance in amplitude\")\n",
    "print(\"Lead 2 has\", np.max(emg[2][0:150000])-np.min(emg[2][0:150000]), \"variance in amplitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If lead 0 was our heart lead we are cooking well. We would expect a difference in ECG and EMG like amplitude on a significant orger of magnitude. Now let's graph it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph part of it to examine\n",
    "x = emg\n",
    "fig, (ax_1,ax_2,ax_3) = plt.subplots(nrows = 3, figsize=(6, 4))\n",
    "ax_1.grid(True)\n",
    "ax_1.plot(x[0][0:150000])\n",
    "ax_1.set(title='leads, samples')\n",
    "ax_2.plot(x[1][0:150000])\n",
    "ax_3.plot(x[2][0:150000])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we graph out components after a cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_cut_file_data = hf.bad_end_cutter_for_samples(\n",
    "        emg,\n",
    "        percent_to_cut=3,\n",
    "        tolerance_percent=5,\n",
    "    )\n",
    "x = re_cut_file_data\n",
    "fig, (ax_1,ax_2,ax_3) = plt.subplots(nrows = 3, figsize=(6, 4))\n",
    "ax_1.grid(True)\n",
    "ax_1.plot(x[0][0:150000])\n",
    "ax_1.set(title='leads, samples')\n",
    "ax_2.plot(x[1][0:150000])\n",
    "ax_3.plot(x[2][0:150000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_cut_file_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do the type of ICA we used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = hf.compute_ICA_two_comp(re_cut_file_data)\n",
    "x = components\n",
    "fig, (ax_1,ax_2,ax_3) = plt.subplots(nrows = 3, figsize=(6, 4))\n",
    "ax_1.grid(True)\n",
    "ax_1.plot(x[0][0:150000])\n",
    "ax_1.set(title='leads, samples')\n",
    "ax_2.plot(x[1][0:150000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at the amplitude differences\n",
    "print(\"ICA components 0 has\", np.max(x[0][0:150000])-np.min(x[0][0:150000]), \"variance in amplitude\")\n",
    "print(\"ICA components 1 has\", np.max(x[1][0:150000])-np.min(x[1][0:150000]), \"variance in amplitude\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WE can see we lost orders of magnitude in our amplitude when we use the ski-kit learn algorith. Let's try a different ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parralell_cut = np.vstack([re_cut_file_data[0],re_cut_file_data[2]])\n",
    "# np.vstack([re_cut_file_data,re_cut_file_data])\n",
    "parralell_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K0, W0, Q = picard(parralell_cut)\n",
    "K1, W1, Y = picard(parralell_cut, ortho=False, random_state=0)\n",
    "K2, W2, Z = picard(parralell_cut, ortho=True, random_state=0)\n",
    "# Picard outputs the whitening matrix, K, \n",
    "# the estimated unmixing matrix, W, and the estimated sources Y. It means that Y = WKX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(nrows = 2,ncols=3, figsize=(12, 4))\n",
    "axis[0,0].grid(True)\n",
    "axis[0,0].plot(Y[0][0:3500])\n",
    "axis[0,0].set(title='Ortho true')\n",
    "axis[1,0].plot(Y[1][0:3500])\n",
    "axis[0,1].grid(True)\n",
    "axis[0,1].plot(Z[0][0:3500])\n",
    "axis[0,1].set(title='Ortho False')\n",
    "axis[1,1].plot(Z[1][0:3500])\n",
    "axis[0,2].grid(True)\n",
    "axis[0,2].plot(Q[0][0:3500])\n",
    "axis[0,2].set(title='Default')\n",
    "axis[1,2].plot(Q[1][0:3500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(nrows = 2,ncols=3, figsize=(12, 4))\n",
    "axis[0,0].grid(True)\n",
    "axis[0,0].plot(Y[0][0:35000])\n",
    "axis[0,0].set(title='Ortho true')\n",
    "axis[1,0].plot(Y[1][0:35000])\n",
    "axis[0,1].grid(True)\n",
    "axis[0,1].plot(Z[0][0:35000])\n",
    "axis[0,1].set(title='Ortho False')\n",
    "axis[1,1].plot(Z[1][0:35000])\n",
    "axis[0,2].grid(True)\n",
    "axis[0,2].plot(Q[0][0:35000])\n",
    "axis[0,2].set(title='Default')\n",
    "axis[1,2].plot(Q[1][0:35000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at the amplitude differences\n",
    "print(\"Piccard ICA, on defaults, components 0 has\", np.max(Q[0][0:150000])-np.min(Q[0][0:150000]), \"variance in amplitude\")\n",
    "print(\"Piccard ICA, on defaults, components 1 has\", np.max(Q[1][0:150000])-np.min(Q[1][0:150000]), \"variance in amplitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So we see a Piccard seperates with a very different amplitude change. We did not check through each parameter. This can be written later once we think about what we 'should' feed it.   On eitehr Picard-0 or Picard (ortho= True or ortho= False) certainly 'more right' in terms of amplitude, but what about the signal quality? Also please run a few files to show it is not normalizing everything to the same amplitudes(but I checked it, and no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One fundamental problem is we need to see how the Picard relates to the real data. This is for scientists to observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.c_[re_cut_file_data[0], re_cut_file_data[2]]\n",
    "X_flip_back = X.T\n",
    "#components = hf.compute_ICA_two_comp(re_cut_file_data)\n",
    "#x = components\n",
    "fig, (ax_1,ax_2,ax_3) = plt.subplots(nrows = 3, figsize=(6, 4))\n",
    "ax_1.grid(True)\n",
    "ax_1.plot(X_flip_back[0])\n",
    "ax_1.set(title='leads, samples')\n",
    "ax_2.plot(X_flip_back[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ica = FastICA(n_components=2)\n",
    "S = ica.fit_transform(X)\n",
    "S_flip_back = S.T\n",
    "fig, (ax_1,ax_2,ax_3) = plt.subplots(nrows = 3, figsize=(6, 4))\n",
    "ax_1.grid(True)\n",
    "ax_1.plot(S_flip_back[0])\n",
    "ax_1.set(title='leads, samples')\n",
    "ax_2.plot(S_flip_back[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropdown to pick ICA possibilities. CUrrently only one -\\o/-\n",
    "ICA_choice = widgets.Dropdown(\n",
    "    options=[\"classic\",\"no_ica_lead3\"],\n",
    "    value='classic',\n",
    "    description=\"Select View Option\",\n",
    "    disabled=False,\n",
    ")\n",
    "display(ICA_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will have to rewrite to accomodate different ICAs, but this is in the future. After we iron out the alternative ICAs. Below we put our EMG data through the pipeline we have now, and we must do picking from an ICA by more peaks or by dis-similarity to the heart/ECG lead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICA_picker_choice = widgets.Dropdown(\n",
    "    options=[\"more_peaks\",\"similar_to_ECG\"],\n",
    "    value='more_peaks',\n",
    "    description=\"Select View Option\",\n",
    "    disabled=False,\n",
    ")\n",
    "display(ICA_picker_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICA_picker_choice.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def working_pipeline_pre_entropy_peaks(our_chosen_samples): \n",
    "    cut_file_data = hf.bad_end_cutter_for_samples(our_chosen_samples, percent_to_cut=3, tolerance_percent=5)\n",
    "    bd_filtered_file_data = hf.emg_bandpass_butter_sample(cut_file_data, 5, 450, 2048, output='sos')\n",
    "    # step 3 end-cutting again to get rid of filtering artifacts\n",
    "    re_cut_file_data = hf.bad_end_cutter_for_samples(bd_filtered_file_data, percent_to_cut=3, tolerance_percent=5)\n",
    "    # skip step4 and do step 5 ICA\n",
    "    components = hf.compute_ICA_two_comp(re_cut_file_data)\n",
    "    #     the picking step!\n",
    "    emg= hf.pick_more_peaks_array(components)\n",
    "    # now process it in final steps\n",
    "    abs_values = abs(emg)\n",
    "    final_envelope_d = hf.emg_highpass_butter(abs_values, 150, 2048)\n",
    "    \n",
    "        \n",
    "    return final_envelope_d\n",
    "\n",
    "def working_pipeline_pre_entropy_ecg(our_chosen_samples): \n",
    "    cut_file_data = hf.bad_end_cutter_for_samples(our_chosen_samples, percent_to_cut=3, tolerance_percent=5)\n",
    "    bd_filtered_file_data = hf.emg_bandpass_butter_sample(cut_file_data, 5, 450, 2048, output='sos')\n",
    "    # step 3 end-cutting again to get rid of filtering artifacts\n",
    "    re_cut_file_data = hf.bad_end_cutter_for_samples(bd_filtered_file_data, percent_to_cut=3, tolerance_percent=5)\n",
    "    # skip step4 and do step 5 ICA\n",
    "    components = hf.compute_ICA_two_comp(re_cut_file_data)\n",
    "    #     the picking step!\n",
    "    emg= hf.pick_lowest_correlation_array(components,re_cut_file_data[0] )\n",
    "    # now process it in final steps\n",
    "    abs_values = abs(emg)\n",
    "    final_envelope_d = hf.emg_highpass_butter(abs_values, 150, 2048)\n",
    "    \n",
    "        \n",
    "    return final_envelope_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's examine our processed EMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ICA_picker_choice.value == 'more_peaks':\n",
    "    processed_data_emg = working_pipeline_pre_entropy_peaks(data_samples)\n",
    "elif ICA_picker_choice.value == 'similar_to_ECG':\n",
    "    processed_data_emg = working_pipeline_pre_entropy_ecg(data_samples)\n",
    "else:\n",
    "    processed_data_emg = working_pipeline_pre_entropy_ecg(data_samples)\n",
    "\n",
    "if time_view == 'Seconds':\n",
    "    %matplotlib inline\n",
    "    # set up plotn\n",
    "    x = processed_data_emg\n",
    "    fig, axis = plt.subplots(nrows = 1, ncols = 2, figsize=(14, 6))\n",
    "    axis[0].grid(True)\n",
    "    axis[0].plot(converted_to_seconds[:len(x)], x)\n",
    "    axis[0].set(title='The whole sample minus last filter lost end')\n",
    "    axis[1].set(title='Your picked area in seconds')\n",
    "    axis[1].grid(True)\n",
    "    axis[1].plot(converted_to_seconds[int(start*emg_fs):int(end*emg_fs)],x[int(start*emg_fs):int(end*emg_fs)])\n",
    "else:\n",
    "    %matplotlib inline\n",
    "    x = processed_data_emg\n",
    "    fig, axis = plt.subplots(nrows = 1, ncols = 2, figsize=(14, 6))\n",
    "    axis[0].grid(True)\n",
    "    axis[0].plot(x)\n",
    "    axis[0].set(title='The whole sample minus last filter lost end')\n",
    "    axis[1].set(title='Your picked area in samples')\n",
    "    axis[1].grid(True)\n",
    "    axis[1].plot(converted_to_samples[int(start):int(end)],x[int(start):int(end)])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we created some basic processed EMG. We will graph it based on the sample selected and the cutoff on entropy. We will ultimately do a cut-off based on something popular in the literature, but let's do one based on one simple parameter first, as an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to select where the cut_off is\n",
    "\n",
    "entropy_cutoff = widgets.Dropdown(\n",
    "    options=[\"Mean\",\"Half_range\"],\n",
    "    value='Mean',\n",
    "    description=\"Select Entropy Cut off\",\n",
    "    disabled=False,\n",
    ")\n",
    "display(entropy_cutoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounded_for_ent(stralist):\n",
    "    rounded= np.round_(stralist, decimals = 5)\n",
    "    return rounded\n",
    "start_s= start * emg_fs\n",
    "end_s = end * emg_fs\n",
    "if time_view == 'Samples':\n",
    "    big_list = rounded_for_ent(processed_data_emg[int(start):int(end)])# replace with whole array of time series!\n",
    "else:\n",
    "   \n",
    "    big_list = rounded_for_ent(processed_data_emg[int(start_s):int(end_s)])\n",
    "slice_length = 100\n",
    "def sliceIterator(lst, sliceLen):\n",
    "    for i in range(len(lst) - sliceLen + 1):\n",
    "        yield lst[i:i + sliceLen]\n",
    "index_hold = []\n",
    "for slice in sliceIterator(big_list, slice_length):\n",
    "    entropy_index = hf.entropical(slice)\n",
    "    index_hold.append(entropy_index)\n",
    "\n",
    "if entropy_cutoff.value == 'Half_range':\n",
    "    decision_cutoff = (np.max(index_hold) + np.min(index_hold))/2\n",
    "else:# entropy_cutoff.value == 'Mean':\n",
    "    decision_cutoff = np.mean(index_hold)\n",
    "\n",
    "\n",
    "rms_rolled = hf.vect_naive_rolling_rms(index_hold,100) # so rms is rms entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we would have split it on that criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_view == 'Samples':\n",
    "    #y= converted_to_samples\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],processed_data_emg[int(start):(int(start) + len(rms_rolled))]*1000)\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],rms_rolled)\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],hf.zero_one_for_jumps_base(rms_rolled,decision_cutoff))\n",
    "    plt.axhline(y = decision_cutoff, color = 'r', linestyle = '-')\n",
    "    \n",
    "else:\n",
    "    y = converted_to_seconds\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s)+int(len(rms_rolled)))], processed_data_emg[int(start_s):(int(start_s)+len(rms_rolled))]*1000)\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s)+int(len(rms_rolled)))],rms_rolled)\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s)+int(len(rms_rolled)))],hf.zero_one_for_jumps_base(rms_rolled,decision_cutoff))\n",
    "    plt.axhline(y = decision_cutoff, color = 'r', linestyle = '-')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above picture the green line represents a 0,1, array which represents the breaths. That picking was based on one simple parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes here!\n",
    "Instead of above code we will do\n",
    "pick breath based on 90% entropy\n",
    "then 50%- this will give start of breath\n",
    "define peak value within 90%\n",
    "moving forward to right 70%-> with smoothing away if there is too short a pause. This is a more complicated algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder\n",
    "time_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rounded_for_ent(stralist):\n",
    "    rounded= np.round_(stralist, decimals = 5)\n",
    "    return rounded\n",
    "if time_view == 'Samples':\n",
    "    big_list = rounded_for_ent(processed_data_emg[int(start):int(end)])# replace with whole array of time series!\n",
    "else:\n",
    "    big_list = rounded_for_ent(processed_data_emg[int(start_s):int(end_s)])\n",
    "slice_length = 100\n",
    "def sliceIterator(lst, sliceLen):\n",
    "    for i in range(len(lst) - sliceLen + 1):\n",
    "        yield lst[i:i + sliceLen]\n",
    "index_hold = []\n",
    "for slice in sliceIterator(big_list, slice_length):\n",
    "    entropy_index = hf.entropical(slice)\n",
    "    index_hold.append(entropy_index)\n",
    "\n",
    "high_decision_cutoff = 0.9  * ((np.max(index_hold)) - (np.min(index_hold))) + np.min(index_hold)\n",
    "decision_cutoff = 0.5 * ((np.max(index_hold)) - (np.min(index_hold))) + np.min(index_hold)\n",
    "\n",
    "rms_rolled = hf.vect_naive_rolling_rms(index_hold,100) # so rms is rms entropy\n",
    "\n",
    "\n",
    "if time_view == 'Samples':\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],processed_data_emg[int(start):(int(start) + len(rms_rolled))]*1000, alpha = 0.5)\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],rms_rolled)\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],hf.zero_one_for_jumps_base(rms_rolled,decision_cutoff))\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],hf.zero_one_for_jumps_base(rms_rolled,high_decision_cutoff), color= 'purple')\n",
    "    plt.axhline(y = decision_cutoff, color = 'r', linestyle = '-')\n",
    "    plt.axhline(y = high_decision_cutoff, color = 'purple', linestyle = '-')\n",
    "    \n",
    "else:\n",
    "    y = converted_to_seconds\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s)+int(len(rms_rolled)))], processed_data_emg[int(start_s):(int(start_s)+len(rms_rolled))]*1000, alpha = 0.5)\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s)+int(len(rms_rolled)))],rms_rolled)\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s)+int(len(rms_rolled)))],hf.zero_one_for_jumps_base(rms_rolled,decision_cutoff))\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s)+int(len(rms_rolled)))],hf.zero_one_for_jumps_base(rms_rolled,high_decision_cutoff), color = 'purple')\n",
    "    plt.axhline(y = decision_cutoff, color = 'r', linestyle = '-')\n",
    "    plt.axhline(y = high_decision_cutoff, color = 'purple', linestyle = '-')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(left, right):\n",
    "    # Initialize an empty list output that will be populated with sorted elements.\n",
    "    # Initialize two variables i and j which are used pointers when iterating through the lists.\n",
    "    output = []\n",
    "    i = j = 0\n",
    "\n",
    "    # Executes the while loop if both pointers i and j are less than the length of the left and right lists\n",
    "    while i < len(left) and j < len(right):\n",
    "        # Compare the elements at every position of both lists during each iteration\n",
    "        if left[i] < right[j]:\n",
    "            # output is populated with the lesser value\n",
    "            output.append(left[i])\n",
    "            # 10. Move pointer to the right\n",
    "            i += 1\n",
    "        else:\n",
    "            output.append(right[j])\n",
    "            j += 1\n",
    "    # The remnant elements are picked from the current pointer value to the end of the respective list\n",
    "    output.extend(left[i:])\n",
    "    output.extend(right[j:])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "slice = builtins.slice\n",
    "\n",
    "class Range(namedtuple('RangeBase', 'start,end')):\n",
    "    \n",
    "    def intersects(self, other):\n",
    "        return (\n",
    "            (self.end >= other.end) and (self.start < other.end) or\n",
    "            (self.end >= other.start) and (self.start < other.start) or\n",
    "            (self.end < other.end) and (self.start >= other.start)\n",
    "        )\n",
    "    \n",
    "    def precedes(self, other):\n",
    "        return self.end < other.start\n",
    "    \n",
    "    def to_slice(self):\n",
    "        return slice(*map(int, self)) # maps whole tuple set\n",
    "    \n",
    "\n",
    "\n",
    "def ranges_of(array):\n",
    "    marks = np.logical_xor(array[1:], array[:-1])\n",
    "    boundaries = np.hstack((np.zeros(1), np.where(marks != 0)[0], np.zeros(1) + len(array) - 1))\n",
    "\n",
    "    if not array[0]:\n",
    "        boundaries = boundaries[1:]\n",
    "    if len(boundaries) % 2 != 0:\n",
    "        boundaries = boundaries[:-1]\n",
    "    return tuple(Range(*boundaries[i:i+2]) for i in range(0, len(boundaries), 2))\n",
    "\n",
    "\n",
    "def intersections(left, right):\n",
    "    i, j = 0, 0\n",
    "    result = []\n",
    "    while i < len(left) and j < len(right):\n",
    "        lelt, relt = left[i], right[j]\n",
    "        if lelt.intersects(relt):\n",
    "            result.append(lelt)\n",
    "            i += 1\n",
    "        elif relt.precedes(lelt):\n",
    "            j += 1\n",
    "        elif lelt.precedes(relt):\n",
    "            i += 1\n",
    "    return result\n",
    "\n",
    "\n",
    "hi = np.array(hf.zero_one_for_jumps_base(rms_rolled, high_decision_cutoff))\n",
    "lo = np.array(hf.zero_one_for_jumps_base(rms_rolled, decision_cutoff))\n",
    "\n",
    "rhi = ranges_of(hi)\n",
    "rlo = ranges_of(lo)\n",
    "\n",
    "keep = intersections(rlo, rhi)\n",
    "\n",
    "\n",
    "points = np.array(sum(keep, start=()), dtype=np.int32)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_line = np.zeros(len(rms_rolled))\n",
    "for seven_range in keep:\n",
    "    seven_line[seven_range.to_slice()] = 7\n",
    "if time_view == 'Samples':\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],processed_data_emg[int(start):(int(start) + len(rms_rolled))]*1000, alpha = 0.5)\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],hf.zero_one_for_jumps_base(rms_rolled,decision_cutoff),  color = 'green')\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],(np.array(hf.zero_one_for_jumps_base(rms_rolled,high_decision_cutoff)))*2, color= 'purple')\n",
    "    #plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))], six_line)\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))], seven_line)\n",
    "else:\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s) + len(rms_rolled))],processed_data_emg[int(start_s):(int(start_s) + len(rms_rolled))]*1000, alpha = 0.5)\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s) + len(rms_rolled))],hf.zero_one_for_jumps_base(rms_rolled,decision_cutoff),  color = 'green')\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s) + len(rms_rolled))],(np.array(hf.zero_one_for_jumps_base(rms_rolled,high_decision_cutoff)))*2, color= 'purple')\n",
    "    #plt.plot(converted_tosecondss[int(star_st):(int(star_st) + len(rms_rolled))], six_line)\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s) + len(rms_rolled))], seven_line)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above our 'seven-line' represents picking based on finding areas with 90% max entropy, then picking evything around them with over 50% entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happened in these breaths?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know the area under the curve for each breath- then the area to where we cross back over 70% of peak values- but should we look at entropy directly- question for Eline. Let's start with breaking the segments into breath or not breath segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our_emg_array = processed_data_emg[int(start):(int(start) + len(rms_rolled))]\n",
    "# jump_indeces = []\n",
    "# zippy = zip(seven_line,seven_line[1:])\n",
    "# for val in enumerate(zippy):\n",
    "#     if val[1][0] != val[1][1]:\n",
    "#         print(val[0])\n",
    "#         jump_indeces.append(val[0])\n",
    "\n",
    "# grouped = np.split(our_emg_array, jump_indeces)\n",
    "# grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to figure out which elements of grouped are breath parts, and make arrays of just those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so grouped needs to be picked down to where at jump indeces we go up from 0 to 7\n",
    "our_emg_array_samples = processed_data_emg[int(start):(int(start) + len(rms_rolled))]\n",
    "our_emg_array_seconds = processed_data_emg[int(start_s):(int(start_s) + len(rms_rolled))]\n",
    "zippy = zip(seven_line,seven_line[1:])\n",
    "breath_indeces = []\n",
    "for val in enumerate(zippy):\n",
    "    if val[1][0] < val[1][1]:\n",
    "        print(val[0])\n",
    "        breath_indeces.append(val[0])\n",
    "if time_view == 'Samples':\n",
    "    grouped_breaths = np.split(our_emg_array_samples, breath_indeces)\n",
    "    grouped_entropy= np.split(rms_rolled, breath_indeces)\n",
    "\n",
    "else:\n",
    "    grouped_breaths = np.split(our_emg_array_seconds, breath_indeces)\n",
    "    grouped_entropy= np.split(rms_rolled, breath_indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in grouped_breaths:\n",
    "    group = abs(group)\n",
    "    plt.plot(group, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in grouped_entropy:\n",
    "     plt.plot(group, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now for each breath started segment get the absolute value, smooth it, and get the area under the curve until we reach 70%\n",
    "The first cell below is an examiner where we plot out the absolute value on each breath group to make sure we are catching breaths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot out the smoothed breaths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_breaths = []\n",
    "for group in grouped_breaths:\n",
    "    group = abs(group)\n",
    "    smoothed = savgol_filter(group, int(0.8* (len(group))), 3)\n",
    "    plt.plot(smoothed)\n",
    "    smoothed_breaths.append(smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot out smoothed entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_breaths_entropy = []\n",
    "for group in grouped_entropy:\n",
    "    smoothed = savgol_filter(group, int(0.8* (len(group))), 3)\n",
    "    plt.plot(smoothed)\n",
    "    smoothed_breaths_entropy.append(smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the area under the curve until the curve of the breath passes it's maximum then hits 70% of it's peak value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_area_under = []\n",
    "for curve in smoothed_breaths:\n",
    "    max_ind = (curve.argmax())\n",
    "    max_val = curve[max_ind]\n",
    "    absolute_val_array = np.abs(curve[max_ind:] - curve.max() * 0.7)\n",
    "    smallest_difference_index = absolute_val_array.argmin()\n",
    "    closest_element = curve[max_ind:][smallest_difference_index]\n",
    "    smallest_difference_index = smallest_difference_index + max_ind\n",
    "    area_under_curve_cut = curve[:smallest_difference_index].sum()\n",
    "    curve_area_under.append(area_under_curve_cut)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the future function pending eline approval \n",
    "def breath_curve_catch(curve):\n",
    "    \"\"\"\n",
    "    The function is intended for smoothed arrays\n",
    "    The function takes a smoothed breath array then calculates part of the area under the curve \n",
    "    including up to the peak and then to 70% of the peak valye\n",
    "    \"\"\"\n",
    "    max_ind = (curve.argmax())\n",
    "    max_val = curve[max_ind]\n",
    "    absolute_val_array = np.abs(curve[max_ind:] - curve.max() * 0.7)\n",
    "    smallest_difference_index = absolute_val_array.argmin()\n",
    "    closest_element = curve[max_ind:][smallest_difference_index]\n",
    "    smallest_difference_index = smallest_difference_index + max_ind\n",
    "    area_under_curve_cut = curve[:smallest_difference_index].sum()\n",
    "    return area_under_curve_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now we have an array with all of those areas (to 70% of curve ) for each smoothed breath\n",
    "curve_area_under"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, but what area these areas? let's see an example of what curve it cuts then takes the area under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_numbers_to_show = []\n",
    "for i in range(len(smoothed_breaths)):\n",
    "    list_of_numbers_to_show.append(str(i))\n",
    "\n",
    "\n",
    "breath_to_show = widgets.Dropdown(\n",
    "    options=list_of_numbers_to_show,\n",
    "    value='1',\n",
    "    description='Picked Breaths:',\n",
    "    disabled=False,\n",
    ")\n",
    "display(breath_to_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's show an example with a graph to show how it looks\n",
    "curve_example = smoothed_breaths[int(breath_to_show.value)]\n",
    "max_ind = (curve_example.argmax())\n",
    "max_val =  curve_example[max_ind]\n",
    "absolute_val_array = np.abs(curve_example[max_ind:] - curve.max() * 0.7)\n",
    "smallest_difference_index = absolute_val_array.argmin()\n",
    "closest_element = curve_example[max_ind:][smallest_difference_index]\n",
    "smallest_difference_index = smallest_difference_index + max_ind\n",
    "area_under_curve_cut = curve_example[:smallest_difference_index].sum()\n",
    "plt.plot(curve_example, color= 'purple', alpha = 0.7)\n",
    "plt.plot(curve_example[:smallest_difference_index],color = 'green', alpha = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what if we want to cut on 70% entropy past the peak instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_area_under_entropy = []\n",
    "\n",
    "for i, entropy_curve in enumerate(smoothed_breaths_entropy):\n",
    "    max_ind = (entropy_curve.argmax())\n",
    "    max_val = entropy_curve[max_ind]\n",
    "    absolute_val_array = np.abs(entropy_curve[max_ind:] - entropy_curve.max() * 0.7)\n",
    "    smallest_difference_index = absolute_val_array.argmin()\n",
    "    closest_element = entropy_curve[max_ind:][smallest_difference_index]\n",
    "    smallest_difference_index = smallest_difference_index + max_ind\n",
    "    area_under_curve_cut = smoothed_breaths[i][:smallest_difference_index].sum()\n",
    "    curve_area_under_entropy.append(area_under_curve_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_area_under_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_emg_array = processed_data_emg[int(start):(int(start) + len(rms_rolled))]\n",
    "our_entropy_array = rms_rolled\n",
    "zippy = zip(seven_line,seven_line[1:])\n",
    "breath_indeces = []\n",
    "for val in enumerate(zippy):\n",
    "    if val[1][0] < val[1][1]:\n",
    "        print(val[0])\n",
    "        breath_indeces.append(val[0])\n",
    "\n",
    "grouped_breaths = np.split(our_emg_array, breath_indeces)\n",
    "grouped_entropy= np.split(rms_rolled, breath_indeces)\n",
    "grouped_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So great, we can do it, however we have to agree that the amount of smoothing to calculate these things is acceptable, and whether we use the entropy or the EMG itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We also need to agree on what other data is collected into the final spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can compare counted 'breaths' to algorithm breaths, then we can compare a sequence of peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_count = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our peak value sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Also we are looking at distance from zero to positive maxima, but the amplitude would be from adjancent low value...\n",
    "We can look at an absolute value array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maxima_in_high_entropy_area(our_array,start=0, end=10000, decision_cutoff='mean'):\n",
    "    \"\"\"\n",
    "    Finds maxima in high entropy areas. You need to have made an rms_rolled variable\n",
    "    on the entropy areas.\n",
    "    The function is not yet optimized, but works here in the notebook. \n",
    "    \"\"\"\n",
    " \n",
    "    #rms_rolled= \n",
    "    decision_array = hf.zero_one_for_jumps_base(rms_rolled,decision_cutoff)\n",
    "    if decision_array[0] == 1:\n",
    "        ups_and_downs = np.logical_xor(decision_array[1:], decision_array[:-1])\n",
    "        indeces_of_boundaries = np.where(ups_and_downs)[0]\n",
    "        maxima = []\n",
    "        boundaries = np.append(\n",
    "            np.append(np.zeros(1), indeces_of_boundaries),\n",
    "            np.zeros(1) + len(our_array),\n",
    "        )\n",
    "        # print(boundaries)\n",
    "        boundaries = boundaries.astype(np.int32)\n",
    "        for slice_start, slice_end in zip(boundaries[::2], boundaries[1::2]):\n",
    "            #print(slice_start, slice_end)\n",
    "            beat = our_array[slice_start:slice_end]\n",
    "            maxima.append(slice_start + np.where(beat == beat.max())[0][0])\n",
    "        maxima_values = our_array[maxima]\n",
    "        # print(maxima_values)\n",
    "        rep_array = np.zeros(len(our_array))\n",
    "        rep_array[maxima] = np.mean(maxima_values)\n",
    "        plt.plot(our_array, alpha = 0.7)\n",
    "        plt.plot(rep_array, alpha = 0.4)\n",
    "    else: \n",
    "        ups_and_downs = np.logical_xor(decision_array[1:], decision_array[:-1])\n",
    "        indeces_of_boundaries = np.where(ups_and_downs)[0]\n",
    "        maxima = []\n",
    "        boundaries = np.append(\n",
    "            indeces_of_boundaries,\n",
    "            np.zeros(1) + len(our_array),\n",
    "        )\n",
    "        boundaries = boundaries.astype(np.int32)\n",
    "        for slice_start, slice_end in zip(boundaries[::2], boundaries[1::2]):\n",
    "            #print(slice_start, slice_end)\n",
    "            beat = our_array[slice_start:slice_end]\n",
    "            maxima.append(slice_start + np.where(beat == beat.max())[0][0])\n",
    "        maxima_values = our_array[maxima]\n",
    "        #print(maxima_values)\n",
    "        rep_array = np.zeros(len(our_array))\n",
    "        rep_array[maxima] = np.mean(maxima_values)\n",
    "        plt.plot(our_array, alpha = 0.7)\n",
    "        plt.plot(rep_array, alpha = 0.4)\n",
    "    return maxima, maxima_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_view == 'Samples':\n",
    "    our_array = processed_data_emg[int(start):int(end)]\n",
    "    starter=int(start)\n",
    "    ender= int(end)\n",
    "else: \n",
    "    our_array = processed_data_emg[int(start_s):int(end_s)]\n",
    "    starter=int(start_s)\n",
    "    ender= int(end_s)\n",
    "\n",
    "maximal , maximal_values = find_maxima_in_high_entropy_area(our_array,start=starter, end=ender, decision_cutoff=decision_cutoff)\n",
    "# note the x axis will be from zero counting up but represent the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several decisions that need to be made here. Note the following:\n",
    "    \n",
    "    We are looking at maxima on the positive. Maybe we should be looking on both sides of zero? Maybe we should be lokking at an absolute value, and then find the maxima? Also note our cut-off on entropy was a bit arbitraty, and gave us an extra breath. These are decisions for the scientific side of a team that need to happen before this interface can go furhter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to decide if we do area under curve for the absolute value, or what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can save off the info on this run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_now = [file_chosen, number_chosen, time_view, start, end, my_count, maximal, maximal_values, curve_area_under]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data_list.append(data_now)\n",
    "big_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_now = [file_chosen, number_chosen, time_view, start, end, my_count, maximal, maximal_values, curve_area_under]\n",
    "# columns_now = ['file_chosen',\n",
    "#                'number_file',\n",
    "#                'units',\n",
    "#                'start_cut',\n",
    "#                'end_cut',\n",
    "#                'my_hand_count',\n",
    "#                'automated_breath_count',\n",
    "#                'maxima',\n",
    "#                'maxima_values',\n",
    "#                'curve_area_under',\n",
    "#                'curve_area_under_entropy',\n",
    "#                ]\n",
    "\n",
    "columns_now = ['file_chosen',\n",
    "               'number_file',\n",
    "               'units',\n",
    "               'start_cut',\n",
    "               'end_cut',\n",
    "               'my_hand_count',\n",
    "               #'automated_breath_count',\n",
    "               'maxima',\n",
    "               'maxima_values',\n",
    "               'curve_area_under',\n",
    "               ]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(big_data_list, columns=columns_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You probably want to change the name to a timestamp, and save every hour, at least. \n",
    "df.to_csv('my_saved_entropy_experiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "447efd04b0c78c5a1c75692111b822003a89ceb18a02b3c39c989880759b7954"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
